import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.stats import entropy
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
import pywt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, confusion_matrix

# Feature extraction functions remain the same
def calculate_stats_features(x):
    return (np.nanmax(x), np.nanmin(x), np.nanmean(x),
            np.nanmedian(x), np.nanvar(x))

def calculate_zero_crossing(x):
    sign_changes = np.signbit(x)
    sign_diff = np.diff(sign_changes)
    return np.count_nonzero(sign_diff)

def calculate_rms(x):
    return np.sqrt(np.nanmean(np.square(x)))

def calculate_entropy(x):
    _, counts = np.unique(x, return_counts=True)
    probabilities = counts / counts.sum()
    return entropy(probabilities)

def get_features(x):
    max_val, min_val, mean, median, var = calculate_stats_features(x)
    zeros_cross = calculate_zero_crossing(x)
    rms = calculate_rms(x)
    ent = calculate_entropy(x)
    return {
        "max": max_val, "min": min_val, "mean": mean,
        "median": median, "var": var,
        "zeros_cross": zeros_cross, "rms": rms,
        "entropy": ent
    }

def get_fourier_coefficients(ecg):
    fft_result = np.fft.fft(ecg)
    fft_mag = np.abs(fft_result)
    return fft_mag[:100]

def get_wavelet_coefficients(ecg):
    coeffs = pywt.wavedec(ecg, 'db4', level=5)
    return np.concatenate(coeffs)[:100]

# PyTorch Dataset
class ECGDataset(Dataset):
    def __init__(self, signals, labels):
        self.signals = torch.FloatTensor(signals)
        self.labels = torch.LongTensor(labels)

    def __len__(self):
        return len(self.signals)

    def __getitem__(self, idx):
        return self.signals[idx], self.labels[idx]

# PyTorch 1D CNN Model
class ECGClassifier(nn.Module):
    def __init__(self, input_channels=1, num_classes=6):
        super(ECGClassifier, self).__init__()

        self.features = nn.Sequential(
            nn.Conv1d(input_channels, 32, kernel_size=5, padding=2),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.MaxPool1d(2),

            nn.Conv1d(32, 64, kernel_size=5, padding=2),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),

            nn.Conv1d(64, 128, kernel_size=5, padding=2),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )

        self.classifier = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Training function
def train_model(model, train_loader, criterion, optimizer, device, num_epochs=50):
    model.train()
    train_losses = []
    train_accs = []

    for epoch in range(num_epochs):
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)

        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')

    return train_losses, train_accs

# Evaluation function
def evaluate_model(model, test_loader, device):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return np.array(all_preds), np.array(all_labels)

# Main execution
if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)

    # Check if CUDA is available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load and prepare data
    df = pd.read_csv("ecg_small_dataset.csv", sep=' ', header=None)
    labels = pd.Series(df.iloc[:, -1])
    df = df.iloc[:, :-1].astype(float)

    # Extract features for Random Forest
    print("Extracting features...")
    features_df = df.apply(lambda x: pd.Series(get_features(x)), axis=1)
    df_fourier = df.apply(lambda x: pd.Series(get_fourier_coefficients(x)), axis=1)
    df_wavelet = df.apply(lambda x: pd.Series(get_wavelet_coefficients(x)), axis=1)
    X_combined = pd.concat([features_df, df_fourier, df_wavelet], axis=1)

    # Train Random Forest
    X_train, X_test, y_train, y_test = train_test_split(
        X_combined, labels, test_size=0.2, random_state=42, stratify=labels)

    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)

    print("\nRandom Forest Results:")
    print(classification_report(y_test, rf_pred))

    # Prepare data for CNN
    X_cnn = np.array(df.values)
    X_cnn = X_cnn.reshape(X_cnn.shape[0], 1, X_cnn.shape[1])  # (batch, channels, length)
    y_cnn = np.array(labels)

    # Split data for CNN
    X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(
        X_cnn, y_cnn, test_size=0.2, random_state=42, stratify=y_cnn)

    # Create datasets and dataloaders
    train_dataset = ECGDataset(X_train_cnn, y_train_cnn)
    test_dataset = ECGDataset(X_test_cnn, y_test_cnn)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Initialize model, loss function, and optimizer
    num_classes = len(np.unique(y_cnn))
    model = ECGClassifier(num_classes=num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Train the model
    print("\nTraining CNN...")
    train_losses, train_accs = train_model(
        model, train_loader, criterion, optimizer, device, num_epochs=50)

    # Evaluate CNN
    cnn_pred, cnn_true = evaluate_model(model, test_loader, device)

    print("\nCNN Results:")
    print(classification_report(cnn_true, cnn_pred))

    # Plot training history
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')

    plt.subplot(1, 2, 2)
    plt.plot(train_accs)
    plt.title('Training Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')

    plt.tight_layout()
    plt.show()